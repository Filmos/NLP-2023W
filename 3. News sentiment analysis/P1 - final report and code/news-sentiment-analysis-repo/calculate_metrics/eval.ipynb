{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook for quantitative evaluation of document and aspect based sentiment analysis using labels assigned manually to articles."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This prepares confusion matrices."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of ABSA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You might need to specify the directory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_absa_model = pd.read_csv(os.path.join('..', 'aspect_based_sentiment_analysis', 'testset_results_absa.csv'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_absa_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The chunk of code below assigns to each article a class based od model output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for i, row in df_test_absa_model.iterrows():\n",
    "    keywords = ast.literal_eval(row.keywords_sentiment)[0]\n",
    "    ners = ast.literal_eval(row.ner_sentiment)[0]\n",
    "    #keywords = [keyword.strip() for keyword in keywords]\n",
    "\n",
    "    aspects = keywords | ners\n",
    "\n",
    "    label = None\n",
    "    score = -1\n",
    "    results = {}\n",
    "    for aspect in aspects.keys():\n",
    "        #print(aspect)\n",
    "        #print(aspects[aspect][0])\n",
    "        for l in aspects[aspect][0]:\n",
    "            if l['score'] > score:\n",
    "                score = l['score']\n",
    "                label = l['label']\n",
    "        #numeric_label = None\n",
    "        if label == 'Negative':\n",
    "            numeric_label = -1\n",
    "        elif label == 'Neutral':\n",
    "            numeric_label = 0\n",
    "        else:\n",
    "            numeric_label = 1\n",
    "        results[aspect] = numeric_label\n",
    "\n",
    "    results_dict[row['Unnamed: 0']] = (results)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_absa_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The chunk of code below reads the data from files filled by annotators. This will be treated as ground truth and evaluated against it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_annotated = pd.DataFrame()\n",
    "for file in os.listdir('./NLP'):\n",
    "    df_test_annotated = pd.concat([df_test_annotated, pd.read_excel('./NLP' + '/' +file)])\n",
    "print(df_test_annotated)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_annotated"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code below is responsible for parsing the text from files prepared by labelers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_annotation = {}\n",
    "results_annotation_overall = {}\n",
    "for i, row in df_test_annotated.iterrows():\n",
    "    keywords = ast.literal_eval(row.keywords_lower)\n",
    "    ners = ast.literal_eval(row.ner_list)\n",
    "\n",
    "    aspects = keywords + ners\n",
    "\n",
    "    results = {}\n",
    "    for aspect in aspects:\n",
    "        results[aspect.split(':')[0]] = aspect.split(':')[1] if ':' in aspect else None\n",
    "\n",
    "    results_annotation[row['Unnamed: 0']] = (results)\n",
    "    results_annotation_overall[row['Unnamed: 0']] = float(row.overall)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_annotation_overall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_annotation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval_class_1_vs_0(annotated, pred, tp, fp, fn, tn):\n",
    "\n",
    "    if annotated == '1' and pred == 1:\n",
    "        tp += 1\n",
    "    if annotated == '1' and pred == 0:\n",
    "        fn += 1\n",
    "    if annotated == '0' and pred == 1:\n",
    "        fp += 1\n",
    "    if annotated == '0' and pred == 0:\n",
    "        tn += 1\n",
    "\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "def eval_class_1_vs_min1(annotated, pred, tp, fp, fn, tn):\n",
    "\n",
    "    if annotated == '1' and pred == 1:\n",
    "        tp += 1\n",
    "    if annotated == '1' and pred == -1:\n",
    "        fn += 1\n",
    "    if annotated == '-1' and pred == 1:\n",
    "        fp += 1\n",
    "    if annotated == '-1' and pred == -1:\n",
    "        tn += 1\n",
    "\n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "def eval_class_0_vs_min1(annotated, pred, tp, fp, fn, tn):\n",
    "\n",
    "    if annotated == '0' and pred == 0:\n",
    "        tp += 1\n",
    "    if annotated == '0' and pred == -1:\n",
    "        fn += 1\n",
    "    if annotated == '-1' and pred == 0:\n",
    "        fp += 1\n",
    "    if annotated == '-1' and pred == -1:\n",
    "        tn += 1\n",
    "\n",
    "    return tp, fp, fn, tn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval_core(results_dict, results_annotation, type):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    for i in results_dict.keys():\n",
    "        for j in results_dict[i].keys():\n",
    "            try:\n",
    "                if results_annotation[i][j] is None:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            if type == '1vs0':\n",
    "                tp, fp, fn, tn = eval_class_1_vs_0(results_annotation[i][j], results_dict[i][j], tp, fp, fn, tn)\n",
    "            elif type == '1vsmin1':\n",
    "                tp, fp, fn, tn = eval_class_1_vs_min1(results_annotation[i][j], results_dict[i][j], tp, fp, fn, tn)\n",
    "            elif type == '0vsmin1':\n",
    "                tp, fp, fn, tn = eval_class_0_vs_min1(results_annotation[i][j], results_dict[i][j], tp, fp, fn, tn)\n",
    "\n",
    "\n",
    "    return tp, fp, fn, tn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tp, fp, fn, tn = eval_core(results_dict, results_annotation, '1vs0')\n",
    "print(tp, fn, fp, tn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tp, fp, fn, tn = eval_core(results_dict, results_annotation, '1vsmin1')\n",
    "print(tp, fn, fp, tn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tp, fp, fn, tn = eval_core(results_dict, results_annotation, '0vsmin1')\n",
    "print(tp, fn, fp, tn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Document based sentiment analysis evaluation.\n",
    "This time the code is much simpler."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read data, you might need to specify the path."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_doc_model = pd.read_csv('../document_based_sentiment_analysis/test_df_overall_sentiment.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_doc_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preparing results as dict."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_doc_model = {}\n",
    "for i, row in df_test_doc_model.iterrows():\n",
    "\n",
    "    results_doc_model[row['Unnamed: 0']] = row.overall_sentiment\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_doc_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_annotation_overall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The rules are a bit different. In document based analysis, the model selected so far did not have the neutral class. Therefore, we treat the neutral class assigned by labelers as positive to be able to compare it. In the future, we will try to switch to a model capable of assigning neutral class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "\n",
    "for k in results_annotation_overall.keys():\n",
    "    if not np.isnan(results_annotation_overall[k]):\n",
    "\n",
    "        if results_doc_model[k] == 1 and results_annotation_overall[k] != -1:\n",
    "            tp += 1\n",
    "        if results_doc_model[k] == 1 and results_annotation_overall[k] == -1:\n",
    "            fp += 1\n",
    "        if results_doc_model[k] == 0 and results_annotation_overall[k] != -1:\n",
    "            fn += 1\n",
    "        if results_doc_model[k] == 0 and results_annotation_overall[k] == -1:\n",
    "            tn += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tp, fn, fp, tn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tp = 0\n",
    "# fp = 0\n",
    "# fn = 0\n",
    "# tn = 0\n",
    "#\n",
    "# for k in results_annotation_overall.keys():\n",
    "#     if not np.isnan(results_annotation_overall[k]):\n",
    "#         #print(results_annotation_overall[k])\n",
    "#         if results_doc_model[k] == 1 and results_annotation_overall[k] == 1:\n",
    "#             tp += 1\n",
    "#         if results_doc_model[k] == 1 and results_annotation_overall[k] != 1:\n",
    "#             fp += 1\n",
    "#         if results_doc_model[k] == 0 and results_annotation_overall[k] == 1:\n",
    "#             fn += 1\n",
    "#         if results_doc_model[k] == 0 and results_annotation_overall[k] != 1:\n",
    "#             tn += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(tp, fn, fp, tn)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
